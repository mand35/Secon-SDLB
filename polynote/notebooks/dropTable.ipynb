{
  "metadata" : {
    "config" : {
      "dependencies" : {
        
      },
      "exclusions" : [
      ],
      "repositories" : [
      ],
      "sparkConfig" : {
        "spark.ui.port" : "4140",
        "spark.sql.catalog.spark_catalog" : "org.apache.spark.sql.delta.catalog.DeltaCatalog",
        "spark.hadoop.javax.jdo.option.ConnectionDriverName" : "org.apache.derby.jdbc.ClientDriver",
        "spark.hadoop.javax.jdo.option.ConnectionPassword" : "1234",
        "spark.hadoop.javax.jdo.option.ConnectionURL" : "jdbc:derby://metastore:1527/db;create=true",
        "spark.hadoop.javax.jdo.option.ConnectionUserName" : "sa",
        "spark.sql.extensions" : "io.delta.sql.DeltaSparkSessionExtension"
      },
      "env" : {
        
      }
    },
    "language_info" : {
      "name" : "scala"
    }
  },
  "nbformat" : 4,
  "nbformat_minor" : 0,
  "cells" : [
    {
      "cell_type" : "markdown",
      "execution_count" : 0,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "# dropTable\n",
        "\n",
        "This is a text cell. Start editing!"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 5,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1656491933171,
          "endTs" : 1656491942135
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "spark.catalog.listTables.show(false)"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "+------------------+--------+-----------+---------+-----------+\n",
            "|name              |database|description|tableType|isTemporary|\n",
            "+------------------+--------+-----------+---------+-----------+\n",
            "|btl_ra_data       |default |null       |EXTERNAL |false      |\n",
            "|raw_tarif_init    |default |null       |EXTERNAL |false      |\n",
            "|raw_tarif_update  |default |null       |EXTERNAL |false      |\n",
            "|stg_aufenthaltsart|default |null       |EXTERNAL |false      |\n",
            "|stg_deckung       |default |null       |EXTERNAL |false      |\n",
            "|stg_ecdetail      |default |null       |EXTERNAL |false      |\n",
            "|stg_eckopf        |default |null       |EXTERNAL |false      |\n",
            "|stg_ecprodukt     |default |null       |EXTERNAL |false      |\n",
            "|stg_familie       |default |null       |EXTERNAL |false      |\n",
            "|stg_gemeinde      |default |null       |EXTERNAL |false      |\n",
            "|stg_leipkopf      |default |null       |EXTERNAL |false      |\n",
            "|stg_leippos       |default |null       |EXTERNAL |false      |\n",
            "|stg_schadenart    |default |null       |EXTERNAL |false      |\n",
            "|stg_tarif_init    |default |null       |EXTERNAL |false      |\n",
            "|stg_tariffaktor   |default |null       |EXTERNAL |false      |\n",
            "|stg_versdeckung   |default |null       |EXTERNAL |false      |\n",
            "|stg_versicherter  |default |null       |EXTERNAL |false      |\n",
            "+------------------+--------+-----------+---------+-----------+\n",
            "\n"
          ],
          "output_type" : "stream"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 1,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1655797675275,
          "endTs" : 1655797679106
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "package org.apache.spark.sql.hive {\r\n",
        "import org.apache.spark.sql.hive.HiveUtils\r\n",
        "import org.apache.spark.SparkContext\r\n",
        "\r\n",
        "object utils {\r\n",
        "    def dropTable(sc: SparkContext, dbName: String, tableName: String, ignoreIfNotExists: Boolean, purge: Boolean): Unit = {\r\n",
        "      HiveUtils\r\n",
        "          .newClientForMetadata(sc.getConf, sc.hadoopConfiguration)\r\n",
        "          .dropTable(dbName, tableName, ignoreIfNotExists, false)\r\n",
        "    }\r\n",
        "  }\r\n",
        "}"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 2,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1655305542353,
          "endTs" : 1655305542426
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "import org.apache.spark.sql.hive.utils\r\n",
        "//utils.dropTable(sc, \"default\", \"stg_tarif_init\", true, true)\r\n",
        "utils.dropTable(\"default\", \"stg_tarif_init\", true, true)"
      ],
      "outputs" : [
        {
          "execution_count" : 2,
          "data" : {
            "application/json" : [
              {
                "pos" : {
                  "sourceId" : "Cell2",
                  "start" : 104,
                  "end" : 160,
                  "point" : 119
                },
                "msg" : "not enough arguments for method dropTable: (sc: org.apache.spark.SparkContext, dbName: String, tableName: String, ignoreIfNotExists: Boolean, purge: Boolean)Unit.\nUnspecified value parameter purge.",
                "severity" : 2
              }
            ],
            "text/plain" : [
              "Error: not enough arguments for method dropTable: (sc: org.apache.spark.SparkContext, dbName: String, tableName: String, ignoreIfNotExists: Boolean, purge: Boolean)Unit.\nUnspecified value parameter purge. (104)"
            ]
          },
          "metadata" : {
            "rel" : "compiler_errors"
          },
          "output_type" : "execute_result"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 3,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1656492048691,
          "endTs" : 1656492049484
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "spark.sql(\"DROP TABLE default.raw_tarif_update\")"
      ],
      "outputs" : [
        {
          "execution_count" : 3,
          "data" : {
            "text/plain" : [
              "[]"
            ]
          },
          "metadata" : {
            "name" : "Out",
            "type" : "DataFrame"
          },
          "output_type" : "execute_result"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 4,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1655305542353,
          "endTs" : 1655305542426
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
      ],
      "outputs" : [
      ]
    }
  ]
}