
dataObjects {
  btl-Familie {
    type = DeltaLakeTableDataObject
    path = "~{id}"
    table = {
      db = "default"
      name = "btl_familie"
    }
  }

}

actions {
  get_familie {
    metadata.feed = businessTramsformation
    type = CustomDataFrameAction
    inputIds = [stg-Familie, stg-Gemeinde]
    outputIds = [btl-Familie]
    transformers = [{
      type = ScalaCodeSparkDfsTransformer
      code = """
	import org.apache.spark.sql.{DataFrame, SparkSession}
  	(session: SparkSession, options: Map[String,String], dfs: Map[String,DataFrame]) => {
          import session.implicits._
          val join_type = "left"
	  val df_fam = dfs("stg-Familie")
	  val df_gem = dfs("stg-Gemeinde")
          val df_res = df_fam
  			.join(df_gem, $"fam_ra_gde_id" === $"gde_id", join_type)
  			.drop("fam_ra_gde_id")
	  		.withColumnRenamed("gde_kt", "Kanton")
  	Map("btl_Familie" -> df_res)
	}
      """
    }]
  }
}
